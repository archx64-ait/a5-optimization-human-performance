{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import config\n",
    "\n",
    "\n",
    "config.HF_DATASETS_CACHE = \"./downloaded_datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mst124974\u001b[0m (\u001b[33mbinit-ait\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dahoas Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(sample: dict) -> dict:\n",
    "     return {\n",
    "        \"prompt\": sample[\"prompt\"].strip(),\n",
    "        \"chosen\": sample[\"chosen\"].strip(),\n",
    "        \"rejected\": sample[\"rejected\"].strip(),\n",
    "    }\n",
    "\n",
    "# load and preprocess dataset\n",
    "def load_dataset_split(split: str, sanity_check: bool = False, cache_dir: str = None) -> Dataset:\n",
    "    dataset = load_dataset(\"Dahoas/rm-static\", split=split, cache_dir=cache_dir)\n",
    "    if sanity_check:\n",
    "        dataset = dataset.select(range(min(len(dataset), 5)))\n",
    "    return dataset.map(preprocess_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check = True\n",
    "train_data = load_dataset_split(\"train\", sanity_check=sanity_check)\n",
    "eval_data = load_dataset_split(\"test\", sanity_check=sanity_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Human: What are some foods that are good for diabetics?\n",
      "\n",
      "Assistant: To be honest, some of these are better than others, and they’re a little more like opinions than facts. For example, many of the diets say to limit vegetables with high sugar content, and there’s some debate on the subject, as far as how much of these vegetables are actually bad for diabetics.\n",
      "\n",
      "Human: Okay, any other advice?\n",
      "\n",
      "Assistant:\n",
      "Chosen Response: What exactly are you asking? There’s a lot of different kinds of diabetic diets. I could try to recommend you some specific foods and recipes. I could help you look up any of the foods, and I could find recipes for them.\n",
      "Rejected Response: Sure, we’ve got information on common mistakes that diabetic patients make with their diets, and even some specific things to do when you eat out and on the go.  One thing that’s recommended in these articles is just to be very mindful of the timing of food intake.\n"
     ]
    }
   ],
   "source": [
    "# debugging to check preprocessed dataset\n",
    "print(\"Prompt:\", train_data[\"prompt\"][1])\n",
    "print(\"Chosen Response:\", train_data[\"chosen\"][1])\n",
    "print(\"Rejected Response:\", train_data[\"rejected\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Human: I am trying to write a fairy tale. What is the most popular plot?\n",
      "\n",
      "Assistant: The most popular plot might be “a princess goes to a faraway land, falls in love, and has a magic genie that grants her wishes”.  We can find plenty of examples of this if we search for fairy tales using the search engine Google.\n",
      "\n",
      "Human: You can't look anything up on google.\n",
      "\n",
      "Assistant: OK, so let’s think about some other popular fairy tales.\n",
      "\n",
      "Human: Do you like the plot of Hansel and Gretel?\n",
      "\n",
      "Assistant: Not particularly.\n",
      "\n",
      "Human: what don't you like?\n",
      "\n",
      "Assistant: I feel like the children do very stupid things.\n",
      "\n",
      "Human: Should I include a witch on my story?\n",
      "\n",
      "Assistant: I think a witch is a fairly common fairy tale element, so yes.  However, what kind of plotline would you like to pursue?\n",
      "\n",
      "Human: I was thinking I might pursue a feminist plot for a more modern fairytale.\n",
      "\n",
      "Assistant: I would be happy to help you make your story a more feminist one.  Are you writing a new story, or retelling a traditional one?\n",
      "\n",
      "Human: I think it would be nice to redo a traditional fairytale and make it feminist.\n",
      "\n",
      "Assistant: Do you want me to help you identify parts of the original fairy tale that you would like to modernize?\n",
      "\n",
      "Human: yes.\n",
      "\n",
      "Assistant: Great! Can you tell me a little bit more about the story that you want to retell?\n",
      "\n",
      "Human: I think I want to retell snow white.\n",
      "\n",
      "Assistant: Oh, that’s a classic!   Have you seen the Disney movie?  It’s very romantic.   It tells the story of how a beautiful princess gets married to a charming prince.\n",
      "\n",
      "Human: yes, except I think I want to have the prince character take the place of the snow white character.\n",
      "\n",
      "Assistant: Gotcha.   So, how does this relate to the plot of the traditional fairy tale?\n",
      "\n",
      "Human: The prince would be stuck in the woods hiding from the witch and waiting for a princess to come for him.\n",
      "\n",
      "Assistant: This would be a really interesting modern retelling.   Would the witch in the new version be a “toxic feminist” or “man hater”?  Or something else?\n",
      "\n",
      "Human: The witch could be trying to make the prince give up his traditionally female characteristics and turn him into a bad person.\n",
      "\n",
      "Assistant:\n",
      "Chosen Response: This sounds like a really interesting modern retelling of the story!\n",
      "Rejected Response: And the prince and the princess both decide that they are more powerful together than apart?\n"
     ]
    }
   ],
   "source": [
    "print(\"Prompt:\", eval_data[\"prompt\"][0])\n",
    "print(\"Chosen Response:\", eval_data[\"chosen\"][0])\n",
    "print(\"Rejected Response:\", eval_data[\"rejected\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model and tokenizer\n",
    "\n",
    "The DPO trainer expects a model of AutoModelForCausalLM, compared to PPO that expects AutoModelForCausalLMWithValueHead for the value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'response', 'chosen', 'rejected'],\n",
      "    num_rows: 5\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'response', 'chosen', 'rejected'],\n",
      "    num_rows: 5\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "learning_rates = [3e-5, 1e-5]\n",
    "betas = [0.1, 0.2]\n",
    "batch_size = 2\n",
    "num_epochs = 3\n",
    "results = []\n",
    "best_loss = float(\"inf\")  # validation loss is smaller than infinity\n",
    "best_model_path = None\n",
    "hyperparameter_combinations = list(itertools.product(learning_rates, betas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\neucl\\Dev\\NLP\\a5-optimization-human-performance\\code\\wandb\\run-20250301_163448-iy5kkwbf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/binit-ait/huggingface/runs/iy5kkwbf' target=\"_blank\">st124974-dpo_lr-3e-05_bs-2_ep-3_beta-0.1</a></strong> to <a href='https://wandb.ai/binit-ait/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/binit-ait/huggingface' target=\"_blank\">https://wandb.ai/binit-ait/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/binit-ait/huggingface/runs/iy5kkwbf' target=\"_blank\">https://wandb.ai/binit-ait/huggingface/runs/iy5kkwbf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 03:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.557057</td>\n",
       "      <td>-0.743484</td>\n",
       "      <td>-1.281764</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.538280</td>\n",
       "      <td>-126.743866</td>\n",
       "      <td>-150.153152</td>\n",
       "      <td>-3.201709</td>\n",
       "      <td>-2.860964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.398400</td>\n",
       "      <td>0.520812</td>\n",
       "      <td>-1.525170</td>\n",
       "      <td>-2.246588</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.721418</td>\n",
       "      <td>-134.560730</td>\n",
       "      <td>-159.801376</td>\n",
       "      <td>-3.280670</td>\n",
       "      <td>-2.917840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.398400</td>\n",
       "      <td>0.530221</td>\n",
       "      <td>-1.799008</td>\n",
       "      <td>-2.523510</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.724502</td>\n",
       "      <td>-137.299118</td>\n",
       "      <td>-162.570602</td>\n",
       "      <td>-3.289494</td>\n",
       "      <td>-2.933159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model found! Saving model at: models/st124974-dpo_lr-3e-05_bs-2_ep-3_beta-0.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 03:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.532556</td>\n",
       "      <td>-3.598132</td>\n",
       "      <td>-5.047195</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.449063</td>\n",
       "      <td>-137.299698</td>\n",
       "      <td>-162.571487</td>\n",
       "      <td>-3.289541</td>\n",
       "      <td>-2.933217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.532555</td>\n",
       "      <td>-3.598191</td>\n",
       "      <td>-5.047298</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.449107</td>\n",
       "      <td>-137.299988</td>\n",
       "      <td>-162.571991</td>\n",
       "      <td>-3.289572</td>\n",
       "      <td>-2.933255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.532561</td>\n",
       "      <td>-3.598219</td>\n",
       "      <td>-5.047341</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.449122</td>\n",
       "      <td>-137.300110</td>\n",
       "      <td>-162.572220</td>\n",
       "      <td>-3.289589</td>\n",
       "      <td>-2.933276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 03:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.513055</td>\n",
       "      <td>-2.473351</td>\n",
       "      <td>-3.381426</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.908075</td>\n",
       "      <td>-144.042526</td>\n",
       "      <td>-171.149765</td>\n",
       "      <td>-3.371381</td>\n",
       "      <td>-3.099240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.531839</td>\n",
       "      <td>-2.875102</td>\n",
       "      <td>-3.856045</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.980943</td>\n",
       "      <td>-148.060043</td>\n",
       "      <td>-175.895950</td>\n",
       "      <td>-3.394658</td>\n",
       "      <td>-3.172605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.539542</td>\n",
       "      <td>-2.978933</td>\n",
       "      <td>-3.967966</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.989033</td>\n",
       "      <td>-149.098358</td>\n",
       "      <td>-177.015152</td>\n",
       "      <td>-3.400038</td>\n",
       "      <td>-3.186575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 03:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.666068</td>\n",
       "      <td>-5.957865</td>\n",
       "      <td>-7.935932</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.978067</td>\n",
       "      <td>-149.098358</td>\n",
       "      <td>-177.015152</td>\n",
       "      <td>-3.400038</td>\n",
       "      <td>-3.186575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666068</td>\n",
       "      <td>-5.957865</td>\n",
       "      <td>-7.935932</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.978067</td>\n",
       "      <td>-149.098358</td>\n",
       "      <td>-177.015152</td>\n",
       "      <td>-3.400038</td>\n",
       "      <td>-3.186575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666068</td>\n",
       "      <td>-5.957865</td>\n",
       "      <td>-7.935932</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.978067</td>\n",
       "      <td>-149.098358</td>\n",
       "      <td>-177.015152</td>\n",
       "      <td>-3.400038</td>\n",
       "      <td>-3.186575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import DPOTrainer, DPOConfig\n",
    "\n",
    "for lr, beta in hyperparameter_combinations:\n",
    "    run_name = f\"st124974-dpo_lr-{lr}_bs-{batch_size}_ep-{num_epochs}_beta-{beta}\"\n",
    "    output_dir = f\"models/{run_name}\"\n",
    "    \n",
    "    # arguments for DPOTrainer\n",
    "    dpo_config = DPOConfig(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=5,\n",
    "        save_total_limit=2,\n",
    "        learning_rate=lr,\n",
    "        report_to=[\"wandb\"],\n",
    "        beta=beta,\n",
    "        run_name=run_name\n",
    "    )\n",
    "\n",
    "    # initialize DPOTrainer\n",
    "    dpo_trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        ref_model=ref_model,\n",
    "        args=dpo_config,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=eval_data,\n",
    "        processing_class=tokenizer,\n",
    "    )\n",
    "\n",
    "    dpo_trainer.train()\n",
    "\n",
    "    #evaluate the model\n",
    "    eval_results = dpo_trainer.evaluate()\n",
    "    loss = eval_results.get(\"eval_loss\", None)\n",
    "    if loss and loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_model_path = output_dir\n",
    "        print(f\"New best model found! Saving model at: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved at: models/st124974-dpo_lr-3e-05_bs-2_ep-3_beta-0.1\n"
     ]
    }
   ],
   "source": [
    "if best_model_path:\n",
    "    model.save_pretrained(best_model_path)\n",
    "    tokenizer.save_pretrained(best_model_path)\n",
    "    print(f\"Model and tokenizer saved at: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository created or already exists: archx64/best-dpo-Qwen-Qwen2-0.5B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06fe899895443c2a918265f71f6b37d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uploaded to Hugging Face Hub: archx64/best-dpo-Qwen-Qwen2-0.5B-Instruct\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "if best_model_path:\n",
    "    api = HfApi()\n",
    "    safe_model_name = model_name.replace(\"/\", \"-\")\n",
    "    model_repo = f\"archx64/best-dpo-{safe_model_name}\"\n",
    "    \n",
    "    # create the repository if it doesn't exist\n",
    "    try:\n",
    "        api.create_repo(repo_id=model_repo, repo_type=\"model\", exist_ok=True)\n",
    "        print(f\"Repository created or already exists: {model_repo}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating repository: {e}\")\n",
    "    \n",
    "    api.upload_folder(folder_path=best_model_path, repo_id=model_repo, repo_type=\"model\")\n",
    "    print(f\"Model uploaded to Hugging Face Hub: {model_repo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running inference on model: archx64/best-dpo-Qwen-Qwen2-0.5B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f051e4974ba4c798aac98465f9bc3c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/758 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb40635ad72a4eafa339d28a77110b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542a1a1c96a14095856bd4283577b872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/256 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a8cacb3bb104054ba4c1304a886e186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314cbfe2fe4b4e3da81ee90d5a1cd09e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aebba3bf557e4709bcaffd2c41c9283e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e111a297d264e0db415292f5a2ec18d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de646aeb9eb47e8bd167c15f12b6849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c65f690646b45cb99f8dc2f24429ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/267 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is the meaning of life?\n",
      "Generated Response: What is the meaning of life? The question of what a living being is and how it should be defined has been a source of philosophical debate for centuries. The answer to this question can be very complex, as there are many different kinds of living beings that could possibly be considered \"living\n"
     ]
    }
   ],
   "source": [
    "# for inference default device is cuda\n",
    "def run_inference(prompt: str, model_name: str, device: str = 'cuda') -> str:\n",
    "    print(f\"running inference on model: {model_name}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(**inputs, max_new_tokens=50)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "if best_model_path:\n",
    "    test_prompt = \"What is the meaning of life?\"\n",
    "    response = run_inference(test_prompt, model_repo, device)\n",
    "    print(f\"Prompt: {test_prompt}\\nGenerated Response: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cuda-12.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
